{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7198a2f-9ca4-45f7-88e6-ae14a29879a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000018D3E46F290>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializando a sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PredictDropout\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verificando se a sessão foi criada\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547012c0-d538-4476-a6d3-a3f9e2e50cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de tabelas\n",
    "tables = [\n",
    "    \"alunos\", \"curriculum\", \"disciplinas_1979\", \"disciplinas_1990\", \"disciplinas_1999\", \"disciplinas_2017\", \"disciplinas_2023\",\n",
    "    \"matriculas\", \"tabela_alunos\", \"tabela_cursos\", \"tabela_dados_ingresso\", \"tabela_dados_pessoais\", \"tabela_disciplinas\", \"tabela_motivo_evasao\"\n",
    "]\n",
    "\n",
    "# Carregando as tabelas\n",
    "dataframes = {table: spark.read.csv(f\"E:/Mestrado UFCG/Semestre 2024.2/Dados/Tabelas_0/{table}.csv\", header=True, inferSchema=True) for table in tables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a633d45-b904-4c9d-91e6-b71cc0d27948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alunos', 'curriculum', 'disciplinas_1979', 'disciplinas_1990', 'disciplinas_1999', 'disciplinas_2017', 'disciplinas_2023', 'matriculas', 'tabela_alunos', 'tabela_cursos', 'tabela_dados_ingresso', 'tabela_dados_pessoais', 'tabela_disciplinas', 'tabela_motivo_evasao']\n"
     ]
    }
   ],
   "source": [
    "print (tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9545eb98-eb31-4bcb-8297-ff4666f3cd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alunos': DataFrame[MATRICULA;ID_CIDADAO;NOME;IDADE;E-MAIL;GENERO;ESTADO_CIVIL_ALUNOS;NACIONALIDADE;LOCAL_NASCIMENTO;ESTADO;TERMO_ESTADO;RAZAO_INATIVIDADE;TIPO_ADMISSAO;TERMO_ADMISSAO;POLITICA_AFIRMATIVA;TIPO_ENSINO_MEDIO;ANO_FORMATURA_ENSINO_MEDIO;CODIGO_CURSO;CODIGO_CURRICULAR; ALUNOS_ATIVOS;EX_ALUNOS;ALUNOS_INATIVOS: string], 'curriculum': DataFrame[CODIGO_CURSO;CODIGO_CURRICULAR;TERMO_NUMERO_MINIMO;TERMO_NUMERO_MAXIMO;NUMERO_MINIMO_CREDITO_INSCRITO;NUMERO_MAXIMO_CREDITOS_INSCRITOS;MINIMO_CREDITO_OBRIGATORIOS_NECESSARIO;MINIMO_CREDITO_OPCIONAIS_NECESSARIOS;MINIMO_CREDITOS_COMPLEMENTARES_NECESSARIOS;MINIMO_ATIVIDADES_EXTENSAO_ACADEMICA: string], 'disciplinas_1979': DataFrame[CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITOS_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL: string], 'disciplinas_1990': DataFrame[CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITOS;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL: string], 'disciplinas_1999': DataFrame[CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CODIGO_DISCIPLINA;HORAS_DISCIPLIN;TIPO;SEMESTRE_IDEAL: string], 'disciplinas_2017': DataFrame[CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITO_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL: string], 'disciplinas_2023': DataFrame[CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITO_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL: string], 'matriculas': DataFrame[MATRICULA;CODIGO_DISCIPLINA;NOME;CREDITOS;HORAS;TERMO;ID_CLASS;NOTA;ESTATUS;TIPO: string], 'tabela_alunos': DataFrame[MATRICULA;ID_CIDADAO;CODE_14102100;TERMO_ADMISSAO_ESTUDANTES;TERMO_ESTADO_ALUNO;ESTADO_ALUNO: string], 'tabela_cursos': DataFrame[CODIGO_CURSO;CODIGO_CURRICULAR;TOTAL_HORA;NUMERO_MINIMO_TERMO;NUMERO_MAXIMO_TERMO: string], 'tabela_dados_ingresso': DataFrame[MATRICULA;TIPO_ADMISSAO;TIPO_ENSINO_MEDIO;POLITICA_AFIRMATIVA_ALUNOS: string], 'tabela_dados_pessoais': DataFrame[MATRICULA;IDADE;GENERO;LOCAL_NASCIMENTO;ESTADO: string], 'tabela_disciplinas': DataFrame[ANO;CODE;DISCIPLINA;ESTADO;CREDITO: string], 'tabela_motivo_evasao': DataFrame[MATRICULA;RAZAO_DE_INATIVIDADE_DE_ALUNO: string]}\n"
     ]
    }
   ],
   "source": [
    "print(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04063265-f137-4c8a-a031-c29be46b3b69",
   "metadata": {},
   "source": [
    "### Trocar delimitador ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe40f14d-2d08-474c-b31d-e7beca16a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando as tabelas com o delimitador correto\n",
    "dataframes = {\n",
    "    table: spark.read.csv(\n",
    "        f\"E:/Mestrado UFCG/Semestre 2024.2/Dados/Tabelas_0/{table}.csv\",\n",
    "        header=True,\n",
    "        sep=\";\",  # Especifica o delimitador como ;\n",
    "        inferSchema=True\n",
    "    )\n",
    "    for table in tables\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e8240ce-d299-4515-b78a-84328883cffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela: alunos\n",
      "Schema:\n",
      "root\n",
      " |-- MATRICULA: integer (nullable = true)\n",
      " |-- ID_CIDADAO: long (nullable = true)\n",
      " |-- NOME: string (nullable = true)\n",
      " |-- IDADE: integer (nullable = true)\n",
      " |-- E-MAIL: string (nullable = true)\n",
      " |-- GENERO: string (nullable = true)\n",
      " |-- ESTADO_CIVIL_ALUNOS: string (nullable = true)\n",
      " |-- NACIONALIDADE: string (nullable = true)\n",
      " |-- LOCAL_NASCIMENTO: string (nullable = true)\n",
      " |-- ESTADO: string (nullable = true)\n",
      " |-- TERMO_ESTADO: double (nullable = true)\n",
      " |-- RAZAO_INATIVIDADE: string (nullable = true)\n",
      " |-- TIPO_ADMISSAO: string (nullable = true)\n",
      " |-- TERMO_ADMISSAO: double (nullable = true)\n",
      " |-- POLITICA_AFIRMATIVA: string (nullable = true)\n",
      " |-- TIPO_ENSINO_MEDIO: string (nullable = true)\n",
      " |-- ANO_FORMATURA_ENSINO_MEDIO: integer (nullable = true)\n",
      " |-- CODIGO_CURSO: integer (nullable = true)\n",
      " |-- CODIGO_CURRICULAR: integer (nullable = true)\n",
      " |--  ALUNOS_ATIVOS: boolean (nullable = true)\n",
      " |-- EX_ALUNOS: boolean (nullable = true)\n",
      " |-- ALUNOS_INATIVOS: boolean (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+---------+----------+-------------------------------+-----+------+---------+-------------------+-------------+-------------------+--------+------------+-----------------+-------------+--------------+-------------------+-----------------+--------------------------+------------+-----------------+--------------+---------+---------------+\n",
      "|MATRICULA|ID_CIDADAO|NOME                           |IDADE|E-MAIL|GENERO   |ESTADO_CIVIL_ALUNOS|NACIONALIDADE|LOCAL_NASCIMENTO   |ESTADO  |TERMO_ESTADO|RAZAO_INATIVIDADE|TIPO_ADMISSAO|TERMO_ADMISSAO|POLITICA_AFIRMATIVA|TIPO_ENSINO_MEDIO|ANO_FORMATURA_ENSINO_MEDIO|CODIGO_CURSO|CODIGO_CURRICULAR| ALUNOS_ATIVOS|EX_ALUNOS|ALUNOS_INATIVOS|\n",
      "+---------+----------+-------------------------------+-----+------+---------+-------------------+-------------+-------------------+--------+------------+-----------------+-------------+--------------+-------------------+-----------------+--------------------------+------------+-----------------+--------------+---------+---------------+\n",
      "|102210001|NULL      |RENATO MACHADO DE SOUSA        |42   |NULL  |MASCULINO|SOLTEIRO           |BRASILEIRA   |NULL               |INATIVO |2002.2      |CANCELAMENTO     |VESTIBULAR   |2002.2        |A0                 |DESCONHECIDA     |NULL                      |14102100    |1999             |false         |false    |true           |\n",
      "|102210002|5175670409|ELISMAEL GUIMARAES MENINO      |43   |NULL  |MASCULINO|SOLTEIRO           |BRASILEIRA   |PIANC? - PB        |GRADUADO|2009.1      |DESCONHECIDO     |VESTIBULAR   |2002.2        |A0                 |DESCONHECIDA     |NULL                      |14102100    |1999             |false         |true     |false          |\n",
      "|102210003|NULL      |EDUARDO JOSE MOREIRA COLACO    |40   |NULL  |MASCULINO|SOLTEIRO           |BRASILEIRA   |CAMPINA GRANDE - PB|INATIVO |2005.1      |REINGRESSO       |VESTIBULAR   |2002.2        |A0                 |DESCONHECIDA     |NULL                      |14102100    |1999             |false         |false    |true           |\n",
      "|102210004|245424530 |ADILTON ANGELO SEIXAS MAGALHAES|43   |NULL  |MASCULINO|SOLTEIRO           |BRASILEIRA   |NULL               |GRADUADO|2004.1      |DESCONHECIDO     |VESTIBULAR   |2002.2        |A0                 |DESCONHECIDA     |NULL                      |14102100    |1999             |false         |true     |false          |\n",
      "|102210005|1195394462|RICARDO MADEIRA FERNANDES      |43   |NULL  |MASCULINO|SOLTEIRO           |BRASILEIRA   |BOA VISTA - RR     |GRADUADO|2007.1      |DESCONHECIDO     |VESTIBULAR   |2002.2        |A0                 |DESCONHECIDA     |NULL                      |14102100    |1999             |false         |true     |false          |\n",
      "+---------+----------+-------------------------------+-----+------+---------+-------------------+-------------+-------------------+--------+------------+-----------------+-------------+--------------+-------------------+-----------------+--------------------------+------------+-----------------+--------------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: curriculum\n",
      "Schema:\n",
      "root\n",
      " |-- CODIGO_CURSO: integer (nullable = true)\n",
      " |-- CODIGO_CURRICULAR: integer (nullable = true)\n",
      " |-- TERMO_NUMERO_MINIMO: integer (nullable = true)\n",
      " |-- TERMO_NUMERO_MAXIMO: integer (nullable = true)\n",
      " |-- NUMERO_MINIMO_CREDITO_INSCRITO: integer (nullable = true)\n",
      " |-- NUMERO_MAXIMO_CREDITOS_INSCRITOS: integer (nullable = true)\n",
      " |-- MINIMO_CREDITO_OBRIGATORIOS_NECESSARIO: integer (nullable = true)\n",
      " |-- MINIMO_CREDITO_OPCIONAIS_NECESSARIOS: integer (nullable = true)\n",
      " |-- MINIMO_CREDITOS_COMPLEMENTARES_NECESSARIOS: integer (nullable = true)\n",
      " |-- MINIMO_ATIVIDADES_EXTENSAO_ACADEMICA: integer (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+------------+-----------------+-------------------+-------------------+------------------------------+--------------------------------+--------------------------------------+------------------------------------+------------------------------------------+------------------------------------+\n",
      "|CODIGO_CURSO|CODIGO_CURRICULAR|TERMO_NUMERO_MINIMO|TERMO_NUMERO_MAXIMO|NUMERO_MINIMO_CREDITO_INSCRITO|NUMERO_MAXIMO_CREDITOS_INSCRITOS|MINIMO_CREDITO_OBRIGATORIOS_NECESSARIO|MINIMO_CREDITO_OPCIONAIS_NECESSARIOS|MINIMO_CREDITOS_COMPLEMENTARES_NECESSARIOS|MINIMO_ATIVIDADES_EXTENSAO_ACADEMICA|\n",
      "+------------+-----------------+-------------------+-------------------+------------------------------+--------------------------------+--------------------------------------+------------------------------------+------------------------------------------+------------------------------------+\n",
      "|14102100    |2017             |9                  |14                 |14                            |24                              |132                                   |56                                  |30                                        |22                                  |\n",
      "|14102100    |2023             |9                  |14                 |16                            |24                              |140                                   |44                                  |34                                        |22                                  |\n",
      "+------------+-----------------+-------------------+-------------------+------------------------------+--------------------------------+--------------------------------------+------------------------------------+------------------------------------------+------------------------------------+\n",
      "\n",
      "\n",
      "\n",
      "Tabela: disciplinas_1979\n",
      "Schema:\n",
      "root\n",
      " |-- CODIGO_CURSO: integer (nullable = true)\n",
      " |-- CODIGO_CURRICULAR: integer (nullable = true)\n",
      " |-- CODIGO_DISCIPLINA: integer (nullable = true)\n",
      " |-- NOME_DISCIPLINA: string (nullable = true)\n",
      " |-- CREDITOS_DISCIPLINA: integer (nullable = true)\n",
      " |-- HORAS_DISCIPLINA: integer (nullable = true)\n",
      " |-- TIPO: string (nullable = true)\n",
      " |-- SEMESTRE_IDEAL: integer (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+------------+-----------------+-----------------+-------------------------+-------------------+----------------+-----------+--------------+\n",
      "|CODIGO_CURSO|CODIGO_CURRICULAR|CODIGO_DISCIPLINA|NOME_DISCIPLINA          |CREDITOS_DISCIPLINA|HORAS_DISCIPLINA|TIPO       |SEMESTRE_IDEAL|\n",
      "+------------+-----------------+-----------------+-------------------------+-------------------+----------------+-----------+--------------+\n",
      "|14102100    |1979             |1108016          |F?SICA PARA COMPUTA??O   |4                  |60              |OBRIGATORIO|-1            |\n",
      "|14102100    |1979             |1109002          |INTRODU??O ? ?LGEBRA     |4                  |60              |OBRIGATORIO|-1            |\n",
      "|14102100    |1979             |1109006          |?LGEBRA LINEAR           |4                  |60              |OBRIGATORIO|-1            |\n",
      "|14102100    |1979             |1109007          |CALCULO DIF E INTEGRAL I |4                  |60              |OBRIGATORIO|-1            |\n",
      "|14102100    |1979             |1109008          |CALCULO DIF E INTEGRAL II|4                  |60              |OBRIGATORIO|-1            |\n",
      "+------------+-----------------+-----------------+-------------------------+-------------------+----------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: disciplinas_1990\n",
      "Schema:\n",
      "root\n",
      " |-- CODIGO_CURSO: integer (nullable = true)\n",
      " |-- CODIGO_CURRICULAR: integer (nullable = true)\n",
      " |-- CODIGO_DISCIPLINA: integer (nullable = true)\n",
      " |-- NOME_DISCIPLINA: string (nullable = true)\n",
      " |-- CREDITOS: integer (nullable = true)\n",
      " |-- HORAS_DISCIPLINA: integer (nullable = true)\n",
      " |-- TIPO: string (nullable = true)\n",
      " |-- SEMESTRE_IDEAL: integer (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+------------+-----------------+-----------------+----------------------+--------+----------------+------------+--------------+\n",
      "|CODIGO_CURSO|CODIGO_CURRICULAR|CODIGO_DISCIPLINA|NOME_DISCIPLINA       |CREDITOS|HORAS_DISCIPLINA|TIPO        |SEMESTRE_IDEAL|\n",
      "+------------+-----------------+-----------------+----------------------+--------+----------------+------------+--------------+\n",
      "|14102100    |1990             |1105149          |GEOMETRIA DESCRITIVA  |4       |60              |OPCIONAL    |-1            |\n",
      "|14102100    |1990             |1108025          |F�SICA GERAL III      |6       |90              |COMPLEMENTAR|-1            |\n",
      "|14102100    |1990             |1108026          |F�SICA EXPERIMENTAL I |3       |60              |COMPLEMENTAR|-1            |\n",
      "|14102100    |1990             |1108027          |F�SICA EXPERIMENTAL II|3       |60              |COMPLEMENTAR|-1            |\n",
      "|14102100    |1990             |1108030          |F�SICA GERAL I        |4       |60              |COMPLEMENTAR|-1            |\n",
      "+------------+-----------------+-----------------+----------------------+--------+----------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: disciplinas_1999\n",
      "Schema:\n",
      "root\n",
      " |-- CODIGO_CURSO: integer (nullable = true)\n",
      " |-- CODIGO_CURRICULAR: integer (nullable = true)\n",
      " |-- CODIGO_DISCIPLINA2: integer (nullable = true)\n",
      " |-- NOME_DISCIPLINA: string (nullable = true)\n",
      " |-- CODIGO_DISCIPLINA4: integer (nullable = true)\n",
      " |-- HORAS_DISCIPLIN: integer (nullable = true)\n",
      " |-- TIPO: string (nullable = true)\n",
      " |-- SEMESTRE_IDEAL: integer (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+------------+-----------------+------------------+------------------------------+------------------+---------------+-----------+--------------+\n",
      "|CODIGO_CURSO|CODIGO_CURRICULAR|CODIGO_DISCIPLINA2|NOME_DISCIPLINA               |CODIGO_DISCIPLINA4|HORAS_DISCIPLIN|TIPO       |SEMESTRE_IDEAL|\n",
      "+------------+-----------------+------------------+------------------------------+------------------+---------------+-----------+--------------+\n",
      "|14102100    |1999             |1105237           |PERCEPCAO DA FORMA            |4                 |60             |OPCIONAL   |-1            |\n",
      "|14102100    |1999             |1108025           |F�SICA GERAL III              |6                 |90             |OBRIGATORIO|-1            |\n",
      "|14102100    |1999             |1108030           |F�SICA GERAL I                |4                 |60             |OBRIGATORIO|-1            |\n",
      "|14102100    |1999             |1108031           |F�SICA GERAL II               |4                 |60             |OBRIGATORIO|-1            |\n",
      "|14102100    |1999             |1108089           |FUNDAMENTOS DE F�SICA CL�SSICA|4                 |60             |OBRIGATORIO|2             |\n",
      "+------------+-----------------+------------------+------------------------------+------------------+---------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: disciplinas_2017\n",
      "Schema:\n",
      "root\n",
      " |-- CODIGO_CURSO: integer (nullable = true)\n",
      " |-- CODIGO_CURRICULAR: integer (nullable = true)\n",
      " |-- CODIGO_DISCIPLINA: integer (nullable = true)\n",
      " |-- NOME_DISCIPLINA: string (nullable = true)\n",
      " |-- CREDITO_DISCIPLINA: integer (nullable = true)\n",
      " |-- HORAS_DISCIPLINA: integer (nullable = true)\n",
      " |-- TIPO: string (nullable = true)\n",
      " |-- SEMESTRE_IDEAL: integer (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+------------+-----------------+-----------------+--------------------------------------+------------------+----------------+--------+--------------+\n",
      "|CODIGO_CURSO|CODIGO_CURRICULAR|CODIGO_DISCIPLINA|NOME_DISCIPLINA                       |CREDITO_DISCIPLINA|HORAS_DISCIPLINA|TIPO    |SEMESTRE_IDEAL|\n",
      "+------------+-----------------+-----------------+--------------------------------------+------------------+----------------+--------+--------------+\n",
      "|14102100    |2017             |1108030          |F�SICA GERAL I                        |4                 |60              |OPCIONAL|-1            |\n",
      "|14102100    |2017             |1108081          |F�SICA GERAL II                       |4                 |60              |OPCIONAL|-1            |\n",
      "|14102100    |2017             |1108100          |F�SICA GERAL III                      |4                 |60              |OPCIONAL|-1            |\n",
      "|14102100    |2017             |1108105          |F�SICA GERAL IV                       |4                 |60              |OPCIONAL|-1            |\n",
      "|14102100    |2017             |1109035          |ALGEBRA VETORIAL E GEOMETRIA ANAL�TICA|4                 |60              |OPCIONAL|-1            |\n",
      "+------------+-----------------+-----------------+--------------------------------------+------------------+----------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: disciplinas_2023\n",
      "Schema:\n",
      "root\n",
      " |-- CODIGO_CURSO: integer (nullable = true)\n",
      " |-- CODIGO_CURRICULAR: integer (nullable = true)\n",
      " |-- CODIGO_DISCIPLINA: integer (nullable = true)\n",
      " |-- NOME_DISCIPLINA: string (nullable = true)\n",
      " |-- CREDITO_DISCIPLINA: integer (nullable = true)\n",
      " |-- HORAS_DISCIPLINA: integer (nullable = true)\n",
      " |-- TIPO: string (nullable = true)\n",
      " |-- SEMESTRE_IDEAL: integer (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+------------+-----------------+-----------------+---------------------------------+------------------+----------------+-----------+--------------+\n",
      "|CODIGO_CURSO|CODIGO_CURRICULAR|CODIGO_DISCIPLINA|NOME_DISCIPLINA                  |CREDITO_DISCIPLINA|HORAS_DISCIPLINA|TIPO       |SEMESTRE_IDEAL|\n",
      "+------------+-----------------+-----------------+---------------------------------+------------------+----------------+-----------+--------------+\n",
      "|14102100    |2023             |1109049          |�LGEBRA LINEAR I                 |4                 |60              |OBRIGATORIO|3             |\n",
      "|14102100    |2023             |1109126          |C�LCULO DIFERENCIAL E INTEGRAL I |4                 |60              |OBRIGATORIO|2             |\n",
      "|14102100    |2023             |1109131          |C�LCULO DIFERENCIAL E INTEGRAL II|4                 |60              |OBRIGATORIO|3             |\n",
      "|14102100    |2023             |1114129          |INTRODU��O � PROBABILIDADE       |4                 |60              |OBRIGATORIO|4             |\n",
      "|14102100    |2023             |1114222          |ESTAT�STICA APLICADA             |4                 |60              |OBRIGATORIO|5             |\n",
      "+------------+-----------------+-----------------+---------------------------------+------------------+----------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: matriculas\n",
      "Schema:\n",
      "root\n",
      " |-- MATRICULA: integer (nullable = true)\n",
      " |-- CODIGO_DISCIPLINA: integer (nullable = true)\n",
      " |-- NOME: string (nullable = true)\n",
      " |-- CREDITOS: integer (nullable = true)\n",
      " |-- HORAS: integer (nullable = true)\n",
      " |-- TERMO: double (nullable = true)\n",
      " |-- ID_CLASS: integer (nullable = true)\n",
      " |-- NOTA: double (nullable = true)\n",
      " |-- ESTATUS: string (nullable = true)\n",
      " |-- TIPO: string (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+---------+-----------------+------------------------+--------+-----+------+--------+----+--------+------+\n",
      "|MATRICULA|CODIGO_DISCIPLINA|NOME                    |CREDITOS|HORAS|TERMO |ID_CLASS|NOTA|ESTATUS |TIPO  |\n",
      "+---------+-----------------+------------------------+--------+-----+------+--------+----+--------+------+\n",
      "|177110007|1307045          |LINGUA PORTUGUESA       |5       |75   |1977.1|NULL    |6.1 |APROVADO|NORMAL|\n",
      "|177110007|1109002          |INTRODU??O ? ?LGEBRA    |4       |60   |1977.2|NULL    |5.2 |APROVADO|NORMAL|\n",
      "|177110007|1304009          |EDUCA??O F?SICA         |2       |30   |1977.2|NULL    |6.0 |APROVADO|NORMAL|\n",
      "|177110007|1307012          |INGLES I                |3       |45   |1977.2|NULL    |8.0 |APROVADO|NORMAL|\n",
      "|177110007|1109007          |CALCULO DIF E INTEGRAL I|4       |60   |1978.1|NULL    |5.1 |APROVADO|NORMAL|\n",
      "+---------+-----------------+------------------------+--------+-----+------+--------+----+--------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: tabela_alunos\n",
      "Schema:\n",
      "root\n",
      " |-- MATRICULA: integer (nullable = true)\n",
      " |-- ID_CIDADAO: long (nullable = true)\n",
      " |-- CODE_14102100: integer (nullable = true)\n",
      " |-- TERMO_ADMISSAO_ESTUDANTES: double (nullable = true)\n",
      " |-- TERMO_ESTADO_ALUNO: double (nullable = true)\n",
      " |-- ESTADO_ALUNO: string (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+---------+----------+-------------+-------------------------+------------------+------------+\n",
      "|MATRICULA|ID_CIDADAO|CODE_14102100|TERMO_ADMISSAO_ESTUDANTES|TERMO_ESTADO_ALUNO|ESTADO_ALUNO|\n",
      "+---------+----------+-------------+-------------------------+------------------+------------+\n",
      "|102210001|NULL      |14102100     |2002.2                   |2002.2            |INATIVO     |\n",
      "|102210002|5175670409|14102100     |2002.2                   |2009.1            |GRADUADO    |\n",
      "|102210003|NULL      |14102100     |2002.2                   |2005.1            |INATIVO     |\n",
      "|102210004|245424530 |14102100     |2002.2                   |2004.1            |GRADUADO    |\n",
      "|102210005|1195394462|14102100     |2002.2                   |2007.1            |GRADUADO    |\n",
      "+---------+----------+-------------+-------------------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: tabela_cursos\n",
      "Schema:\n",
      "root\n",
      " |-- CODIGO_CURSO: integer (nullable = true)\n",
      " |-- CODIGO_CURRICULAR: integer (nullable = true)\n",
      " |-- TOTAL_HORA: integer (nullable = true)\n",
      " |-- NUMERO_MINIMO_TERMO: integer (nullable = true)\n",
      " |-- NUMERO_MAXIMO_TERMO: integer (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+------------+-----------------+----------+-------------------+-------------------+\n",
      "|CODIGO_CURSO|CODIGO_CURRICULAR|TOTAL_HORA|NUMERO_MINIMO_TERMO|NUMERO_MAXIMO_TERMO|\n",
      "+------------+-----------------+----------+-------------------+-------------------+\n",
      "|14102100    |2017             |3270      |9                  |14                 |\n",
      "|14102100    |2023             |3270      |9                  |14                 |\n",
      "+------------+-----------------+----------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "\n",
      "Tabela: tabela_dados_ingresso\n",
      "Schema:\n",
      "root\n",
      " |-- MATRICULA: integer (nullable = true)\n",
      " |-- TIPO_ADMISSAO: string (nullable = true)\n",
      " |-- TIPO_ENSINO_MEDIO: string (nullable = true)\n",
      " |-- POLITICA_AFIRMATIVA_ALUNOS: string (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+---------+-------------+-----------------+--------------------------+\n",
      "|MATRICULA|TIPO_ADMISSAO|TIPO_ENSINO_MEDIO|POLITICA_AFIRMATIVA_ALUNOS|\n",
      "+---------+-------------+-----------------+--------------------------+\n",
      "|102210001|VESTIBULAR   |DESCONHECIDA     |A0                        |\n",
      "|102210002|VESTIBULAR   |DESCONHECIDA     |A0                        |\n",
      "|102210003|VESTIBULAR   |DESCONHECIDA     |A0                        |\n",
      "|102210004|VESTIBULAR   |DESCONHECIDA     |A0                        |\n",
      "|102210005|VESTIBULAR   |DESCONHECIDA     |A0                        |\n",
      "+---------+-------------+-----------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: tabela_dados_pessoais\n",
      "Schema:\n",
      "root\n",
      " |-- MATRICULA: integer (nullable = true)\n",
      " |-- IDADE: integer (nullable = true)\n",
      " |-- GENERO: string (nullable = true)\n",
      " |-- LOCAL_NASCIMENTO: string (nullable = true)\n",
      " |-- ESTADO: string (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+---------+-----+---------+----------------+------+\n",
      "|MATRICULA|IDADE|GENERO   |LOCAL_NASCIMENTO|ESTADO|\n",
      "+---------+-----+---------+----------------+------+\n",
      "|102210001|42   |MASCULINO|NULL            |NULL  |\n",
      "|102210002|43   |MASCULINO|PIANC?          |PB    |\n",
      "|102210003|40   |MASCULINO|CAMPINA GRANDE  |PB    |\n",
      "|102210004|43   |MASCULINO|NULL            |NULL  |\n",
      "|102210005|43   |MASCULINO|BOA VISTA       |RR    |\n",
      "+---------+-----+---------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: tabela_disciplinas\n",
      "Schema:\n",
      "root\n",
      " |-- ANO: string (nullable = true)\n",
      " |-- CODE: string (nullable = true)\n",
      " |-- DISCIPLINA: string (nullable = true)\n",
      " |-- ESTADO: string (nullable = true)\n",
      " |-- CREDITO: string (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+----+-------+-------------------------+-----------+-------+\n",
      "|ANO |CODE   |DISCIPLINA               |ESTADO     |CREDITO|\n",
      "+----+-------+-------------------------+-----------+-------+\n",
      "|1979|1108016|F?SICA PARA COMPUTA??O   |OBRIGATORIO|4      |\n",
      "|1979|1109002|INTRODU??O ? ?LGEBRA     |OBRIGATORIO|4      |\n",
      "|1979|1109006|?LGEBRA LINEAR           |OBRIGATORIO|4      |\n",
      "|1979|1109007|CALCULO DIF E INTEGRAL I |OBRIGATORIO|4      |\n",
      "|1979|1109008|CALCULO DIF E INTEGRAL II|OBRIGATORIO|4      |\n",
      "+----+-------+-------------------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Tabela: tabela_motivo_evasao\n",
      "Schema:\n",
      "root\n",
      " |-- MATRICULA: integer (nullable = true)\n",
      " |-- RAZAO_DE_INATIVIDADE_DE_ALUNO: string (nullable = true)\n",
      "\n",
      "Primeiras linhas:\n",
      "+---------+-----------------------------+\n",
      "|MATRICULA|RAZAO_DE_INATIVIDADE_DE_ALUNO|\n",
      "+---------+-----------------------------+\n",
      "|102210001|CANCELAMENTO                 |\n",
      "|102210002|DESCONHECIDO                 |\n",
      "|102210003|REINGRESSO                   |\n",
      "|102210004|DESCONHECIDO                 |\n",
      "|102210005|DESCONHECIDO                 |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for table, df in dataframes.items():\n",
    "    print(f\"Tabela: {table}\")\n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    print(\"Primeiras linhas:\")\n",
    "    df.show(5, truncate=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c98df6-8413-45e2-9db1-ca43d11776da",
   "metadata": {},
   "source": [
    "### Visualizar a base como ficou ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d14fe46-cd5f-4d12-b475-1c96fb88a933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alunos': DataFrame[MATRICULA: int, ID_CIDADAO: bigint, NOME: string, IDADE: int, E-MAIL: string, GENERO: string, ESTADO_CIVIL_ALUNOS: string, NACIONALIDADE: string, LOCAL_NASCIMENTO: string, ESTADO: string, TERMO_ESTADO: double, RAZAO_INATIVIDADE: string, TIPO_ADMISSAO: string, TERMO_ADMISSAO: double, POLITICA_AFIRMATIVA: string, TIPO_ENSINO_MEDIO: string, ANO_FORMATURA_ENSINO_MEDIO: int, CODIGO_CURSO: int, CODIGO_CURRICULAR: int,  ALUNOS_ATIVOS: boolean, EX_ALUNOS: boolean, ALUNOS_INATIVOS: boolean], 'curriculum': DataFrame[CODIGO_CURSO: int, CODIGO_CURRICULAR: int, TERMO_NUMERO_MINIMO: int, TERMO_NUMERO_MAXIMO: int, NUMERO_MINIMO_CREDITO_INSCRITO: int, NUMERO_MAXIMO_CREDITOS_INSCRITOS: int, MINIMO_CREDITO_OBRIGATORIOS_NECESSARIO: int, MINIMO_CREDITO_OPCIONAIS_NECESSARIOS: int, MINIMO_CREDITOS_COMPLEMENTARES_NECESSARIOS: int, MINIMO_ATIVIDADES_EXTENSAO_ACADEMICA: int], 'disciplinas_1979': DataFrame[CODIGO_CURSO: int, CODIGO_CURRICULAR: int, CODIGO_DISCIPLINA: int, NOME_DISCIPLINA: string, CREDITOS_DISCIPLINA: int, HORAS_DISCIPLINA: int, TIPO: string, SEMESTRE_IDEAL: int], 'disciplinas_1990': DataFrame[CODIGO_CURSO: int, CODIGO_CURRICULAR: int, CODIGO_DISCIPLINA: int, NOME_DISCIPLINA: string, CREDITOS: int, HORAS_DISCIPLINA: int, TIPO: string, SEMESTRE_IDEAL: int], 'disciplinas_1999': DataFrame[CODIGO_CURSO: int, CODIGO_CURRICULAR: int, CODIGO_DISCIPLINA2: int, NOME_DISCIPLINA: string, CODIGO_DISCIPLINA4: int, HORAS_DISCIPLIN: int, TIPO: string, SEMESTRE_IDEAL: int], 'disciplinas_2017': DataFrame[CODIGO_CURSO: int, CODIGO_CURRICULAR: int, CODIGO_DISCIPLINA: int, NOME_DISCIPLINA: string, CREDITO_DISCIPLINA: int, HORAS_DISCIPLINA: int, TIPO: string, SEMESTRE_IDEAL: int], 'disciplinas_2023': DataFrame[CODIGO_CURSO: int, CODIGO_CURRICULAR: int, CODIGO_DISCIPLINA: int, NOME_DISCIPLINA: string, CREDITO_DISCIPLINA: int, HORAS_DISCIPLINA: int, TIPO: string, SEMESTRE_IDEAL: int], 'matriculas': DataFrame[MATRICULA: int, CODIGO_DISCIPLINA: int, NOME: string, CREDITOS: int, HORAS: int, TERMO: double, ID_CLASS: int, NOTA: double, ESTATUS: string, TIPO: string], 'tabela_alunos': DataFrame[MATRICULA: int, ID_CIDADAO: bigint, CODE_14102100: int, TERMO_ADMISSAO_ESTUDANTES: double, TERMO_ESTADO_ALUNO: double, ESTADO_ALUNO: string], 'tabela_cursos': DataFrame[CODIGO_CURSO: int, CODIGO_CURRICULAR: int, TOTAL_HORA: int, NUMERO_MINIMO_TERMO: int, NUMERO_MAXIMO_TERMO: int], 'tabela_dados_ingresso': DataFrame[MATRICULA: int, TIPO_ADMISSAO: string, TIPO_ENSINO_MEDIO: string, POLITICA_AFIRMATIVA_ALUNOS: string], 'tabela_dados_pessoais': DataFrame[MATRICULA: int, IDADE: int, GENERO: string, LOCAL_NASCIMENTO: string, ESTADO: string], 'tabela_disciplinas': DataFrame[ANO: string, CODE: string, DISCIPLINA: string, ESTADO: string, CREDITO: string], 'tabela_motivo_evasao': DataFrame[MATRICULA: int, RAZAO_DE_INATIVIDADE_DE_ALUNO: string]}\n"
     ]
    }
   ],
   "source": [
    "print(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f0ba0-a4e7-46e1-8c5e-f59a3106ba4f",
   "metadata": {},
   "source": [
    "### Funções de Limpeza ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68633db4-ed10-44cb-be5d-62ff12683285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean, when, trim, lower, min, max\n",
    "\n",
    "# Função para tratamento de valores ausentes\n",
    "def handle_missing_values(df, strategy=\"mean\"):\n",
    "    for column in df.columns:\n",
    "        if df.select(column).distinct().count() > 1:  # Evita erro com colunas únicas\n",
    "            if strategy == \"mean\" and df.schema[column].dataType in ['int', 'double', 'float']:\n",
    "                mean_value = df.select(mean(col(column))).collect()[0][0]\n",
    "                df = df.withColumn(column, when(col(column).isNull(), mean_value).otherwise(col(column)))\n",
    "            elif strategy == \"median\" and df.schema[column].dataType in ['int', 'double', 'float']:\n",
    "                median_value = df.approxQuantile(column, [0.5], 0.01)[0]\n",
    "                df = df.withColumn(column, when(col(column).isNull(), median_value).otherwise(col(column)))\n",
    "            elif strategy == \"mode\":\n",
    "                mode_value = df.groupBy(column).count().orderBy(col(\"count\").desc()).first()[0]\n",
    "                df = df.withColumn(column, when(col(column).isNull(), mode_value).otherwise(col(column)))\n",
    "            elif strategy == \"drop\":\n",
    "                df = df.na.drop()\n",
    "    return df\n",
    "\n",
    "# Função para remoção de outliers\n",
    "def remove_outliers(df, column):\n",
    "    if df.schema[column].dataType in ['int', 'double', 'float']:\n",
    "        quantiles = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "        q1, q3 = quantiles[0], quantiles[1]\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        df = df.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "    return df\n",
    "\n",
    "# Função para padronização de categorias\n",
    "def standardize_categories(df, column):\n",
    "    df = df.withColumn(column, col(column).cast(\"string\"))\n",
    "    df = df.withColumn(column, trim(lower(col(column))))  # Remove espaços e converte para minúsculas\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeca964-25bd-47f3-8539-cbec77ae0145",
   "metadata": {},
   "source": [
    "### Aplicação das Transformações ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "601ffe86-c492-46fc-a13e-571c5544bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando tabela: alunos\n",
      "Erro ao salvar a tabela alunos: An error occurred while calling o904.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: curriculum\n",
      "Erro ao salvar a tabela curriculum: An error occurred while calling o1040.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: disciplinas_1979\n",
      "Erro ao salvar a tabela disciplinas_1979: An error occurred while calling o1150.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: disciplinas_1990\n",
      "Erro ao salvar a tabela disciplinas_1990: An error occurred while calling o1260.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: disciplinas_1999\n",
      "Erro ao salvar a tabela disciplinas_1999: An error occurred while calling o1370.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: disciplinas_2017\n",
      "Erro ao salvar a tabela disciplinas_2017: An error occurred while calling o1480.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: disciplinas_2023\n",
      "Erro ao salvar a tabela disciplinas_2023: An error occurred while calling o1590.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: matriculas\n",
      "Erro ao salvar a tabela matriculas: An error occurred while calling o1726.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: tabela_alunos\n",
      "Erro ao salvar a tabela tabela_alunos: An error occurred while calling o1810.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: tabela_cursos\n",
      "Erro ao salvar a tabela tabela_cursos: An error occurred while calling o1881.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: tabela_dados_ingresso\n",
      "Erro ao salvar a tabela tabela_dados_ingresso: An error occurred while calling o1939.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: tabela_dados_pessoais\n",
      "Erro ao salvar a tabela tabela_dados_pessoais: An error occurred while calling o2010.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: tabela_disciplinas\n",
      "Erro ao salvar a tabela tabela_disciplinas: An error occurred while calling o2081.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processando tabela: tabela_motivo_evasao\n",
      "Erro ao salvar a tabela tabela_motivo_evasao: An error occurred while calling o2113.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "\n",
      "Processamento concluído! As tabelas limpas foram salvas com o prefixo 'cleaned_'.\n"
     ]
    }
   ],
   "source": [
    "# Aplicando as transformações nas tabelas\n",
    "for table, df in dataframes.items():\n",
    "    print(f\"Processando tabela: {table}\")\n",
    "    \n",
    "    # Tratamento de valores ausentes\n",
    "    df = handle_missing_values(df, strategy=\"mean\")\n",
    "    \n",
    "    # Remoção de outliers e padronização de categorias\n",
    "    for col_name in df.columns:\n",
    "        if df.schema[col_name].dataType in ['int', 'double', 'float']:\n",
    "            df = remove_outliers(df, col_name)  # Remoção de outliers\n",
    "        else:\n",
    "            df = standardize_categories(df, col_name)  # Padronização de categorias\n",
    "    \n",
    "    # Remoção de duplicatas\n",
    "    df = df.dropDuplicates()\n",
    "    \n",
    "    # Salvando a tabela limpa no novo local\n",
    "    try:\n",
    "        df.write.mode(\"overwrite\").csv(f\"file:///C:/Users/Big Data/Documents/databrikcs/tabelas_tratadas/cleaned_{table}\", header=True)\n",
    "        print(f\"Tabela {table} processada e salva com sucesso!\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar a tabela {table}: {e}\\n\")\n",
    "\n",
    "print(\"Processamento concluído! As tabelas limpas foram salvas com o prefixo 'cleaned_'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e94281b-128f-43c9-b4f1-8096f7b90296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando tabela: alunos\n",
      "Verificando DataFrame: alunos\n",
      "root\n",
      " |-- MATRICULA;ID_CIDADAO;NOME;IDADE;E-MAIL;GENERO;ESTADO_CIVIL_ALUNOS;NACIONALIDADE;LOCAL_NASCIMENTO;ESTADO;TERMO_ESTADO;RAZAO_INATIVIDADE;TIPO_ADMISSAO;TERMO_ADMISSAO;POLITICA_AFIRMATIVA;TIPO_ENSINO_MEDIO;ANO_FORMATURA_ENSINO_MEDIO;CODIGO_CURSO;CODIGO_CURRICULAR; ALUNOS_ATIVOS;EX_ALUNOS;ALUNOS_INATIVOS: string (nullable = true)\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|MATRICULA;ID_CIDADAO;NOME;IDADE;E-MAIL;GENERO;ESTADO_CIVIL_ALUNOS;NACIONALIDADE;LOCAL_NASCIMENTO;ESTADO;TERMO_ESTADO;RAZAO_INATIVIDADE;TIPO_ADMISSAO;TERMO_ADMISSAO;POLITICA_AFIRMATIVA;TIPO_ENSINO_MEDIO;ANO_FORMATURA_ENSINO_MEDIO;CODIGO_CURSO;CODIGO_CURRICULAR; ALUNOS_ATIVOS;EX_ALUNOS;ALUNOS_INATIVOS|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                                                                                                        107110033;9963972...|\n",
      "|                                                                                                                                                                                                                                                                                        107210032;6518425...|\n",
      "|                                                                                                                                                                                                                                                                                        109110072;6433169...|\n",
      "|                                                                                                                                                                                                                                                                                        109210039;8383311...|\n",
      "|                                                                                                                                                                                                                                                                                        115111577;8550390...|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 3761\n",
      "Erro ao salvar a tabela alunos: An error occurred while calling o151.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: curriculum\n",
      "Verificando DataFrame: curriculum\n",
      "root\n",
      " |-- CODIGO_CURSO;CODIGO_CURRICULAR;TERMO_NUMERO_MINIMO;TERMO_NUMERO_MAXIMO;NUMERO_MINIMO_CREDITO_INSCRITO;NUMERO_MAXIMO_CREDITOS_INSCRITOS;MINIMO_CREDITO_OBRIGATORIOS_NECESSARIO;MINIMO_CREDITO_OPCIONAIS_NECESSARIOS;MINIMO_CREDITOS_COMPLEMENTARES_NECESSARIOS;MINIMO_ATIVIDADES_EXTENSAO_ACADEMICA: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CODIGO_CURSO;CODIGO_CURRICULAR;TERMO_NUMERO_MINIMO;TERMO_NUMERO_MAXIMO;NUMERO_MINIMO_CREDITO_INSCRITO;NUMERO_MAXIMO_CREDITOS_INSCRITOS;MINIMO_CREDITO_OBRIGATORIOS_NECESSARIO;MINIMO_CREDITO_OPCIONAIS_NECESSARIOS;MINIMO_CREDITOS_COMPLEMENTARES_NECESSARIOS;MINIMO_ATIVIDADES_EXTENSAO_ACADEMICA|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                                                                                              14102100;2017;9;1...|\n",
      "|                                                                                                                                                                                                                                                                              14102100;2023;9;1...|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Número de linhas: 2\n",
      "Erro ao salvar a tabela curriculum: An error occurred while calling o172.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: disciplinas_1979\n",
      "Verificando DataFrame: disciplinas_1979\n",
      "root\n",
      " |-- CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITOS_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL: string (nullable = true)\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITOS_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL|\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                     14102100;1979;110...|\n",
      "|                                                                                                     14102100;1979;130...|\n",
      "|                                                                                                     14102100;1979;130...|\n",
      "|                                                                                                     14102100;1979;141...|\n",
      "|                                                                                                     14102100;1979;130...|\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 79\n",
      "Erro ao salvar a tabela disciplinas_1979: An error occurred while calling o193.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: disciplinas_1990\n",
      "Verificando DataFrame: disciplinas_1990\n",
      "root\n",
      " |-- CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITOS;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITOS;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL|\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                          14102100;1990;141...|\n",
      "|                                                                                          14102100;1990;141...|\n",
      "|                                                                                          14102100;1990;141...|\n",
      "|                                                                                          14102100;1990;110...|\n",
      "|                                                                                          14102100;1990;141...|\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 151\n",
      "Erro ao salvar a tabela disciplinas_1990: An error occurred while calling o214.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: disciplinas_1999\n",
      "Verificando DataFrame: disciplinas_1999\n",
      "root\n",
      " |-- CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CODIGO_DISCIPLINA;HORAS_DISCIPLIN;TIPO;SEMESTRE_IDEAL: string (nullable = true)\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CODIGO_DISCIPLINA;HORAS_DISCIPLIN;TIPO;SEMESTRE_IDEAL|\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                  14102100;1999;130...|\n",
      "|                                                                                                  14102100;1999;141...|\n",
      "|                                                                                                  14102100;1999;141...|\n",
      "|                                                                                                  14102100;1999;111...|\n",
      "|                                                                                                  14102100;1999;130...|\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 238\n",
      "Erro ao salvar a tabela disciplinas_1999: An error occurred while calling o235.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: disciplinas_2017\n",
      "Verificando DataFrame: disciplinas_2017\n",
      "root\n",
      " |-- CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITO_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL: string (nullable = true)\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITO_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                    14102100;2017;130...|\n",
      "|                                                                                                    14102100;2017;110...|\n",
      "|                                                                                                    14102100;2017;141...|\n",
      "|                                                                                                    14102100;2017;141...|\n",
      "|                                                                                                    14102100;2017;141...|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 133\n",
      "Erro ao salvar a tabela disciplinas_2017: An error occurred while calling o256.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: disciplinas_2023\n",
      "Verificando DataFrame: disciplinas_2023\n",
      "root\n",
      " |-- CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITO_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL: string (nullable = true)\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|CODIGO_CURSO;CODIGO_CURRICULAR;CODIGO_DISCIPLINA;NOME_DISCIPLINA;CREDITO_DISCIPLINA;HORAS_DISCIPLINA;TIPO;SEMESTRE_IDEAL|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                    14102100;2023;130...|\n",
      "|                                                                                                    14102100;2023;141...|\n",
      "|                                                                                                    14102100;2023;141...|\n",
      "|                                                                                                    14102100;2023;141...|\n",
      "|                                                                                                    14102100;2023;141...|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 88\n",
      "Erro ao salvar a tabela disciplinas_2023: An error occurred while calling o277.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: matriculas\n",
      "Verificando DataFrame: matriculas\n",
      "root\n",
      " |-- MATRICULA;CODIGO_DISCIPLINA;NOME;CREDITOS;HORAS;TERMO;ID_CLASS;NOTA;ESTATUS;TIPO: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------+\n",
      "|MATRICULA;CODIGO_DISCIPLINA;NOME;CREDITOS;HORAS;TERMO;ID_CLASS;NOTA;ESTATUS;TIPO|\n",
      "+--------------------------------------------------------------------------------+\n",
      "|                                                            177210090;1411048...|\n",
      "|                                                            177210102;1411011...|\n",
      "|                                                            177210155;1305036...|\n",
      "|                                                            177210160;1303021...|\n",
      "|                                                            177210272;1305040...|\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 182660\n",
      "Erro ao salvar a tabela matriculas: An error occurred while calling o298.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: tabela_alunos\n",
      "Verificando DataFrame: tabela_alunos\n",
      "root\n",
      " |-- MATRICULA;ID_CIDADAO;CODE_14102100;TERMO_ADMISSAO_ESTUDANTES;TERMO_ESTADO_ALUNO;ESTADO_ALUNO: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------------------+\n",
      "|MATRICULA;ID_CIDADAO;CODE_14102100;TERMO_ADMISSAO_ESTUDANTES;TERMO_ESTADO_ALUNO;ESTADO_ALUNO|\n",
      "+--------------------------------------------------------------------------------------------+\n",
      "|                                                                        103110393;4628663...|\n",
      "|                                                                        106110143;7087404...|\n",
      "|                                                                        106210157;7799224...|\n",
      "|                                                                        106210603;7696220...|\n",
      "|                                                                        108210282;5979539...|\n",
      "+--------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 3761\n",
      "Erro ao salvar a tabela tabela_alunos: An error occurred while calling o319.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: tabela_cursos\n",
      "Verificando DataFrame: tabela_cursos\n",
      "root\n",
      " |-- CODIGO_CURSO;CODIGO_CURRICULAR;TOTAL_HORA;NUMERO_MINIMO_TERMO;NUMERO_MAXIMO_TERMO: string (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------------------------------+\n",
      "|CODIGO_CURSO;CODIGO_CURRICULAR;TOTAL_HORA;NUMERO_MINIMO_TERMO;NUMERO_MAXIMO_TERMO|\n",
      "+---------------------------------------------------------------------------------+\n",
      "|                                                             14102100;2023;327...|\n",
      "|                                                             14102100;2017;327...|\n",
      "+---------------------------------------------------------------------------------+\n",
      "\n",
      "Número de linhas: 2\n",
      "Erro ao salvar a tabela tabela_cursos: An error occurred while calling o340.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: tabela_dados_ingresso\n",
      "Verificando DataFrame: tabela_dados_ingresso\n",
      "root\n",
      " |-- MATRICULA;TIPO_ADMISSAO;TIPO_ENSINO_MEDIO;POLITICA_AFIRMATIVA_ALUNOS: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------+\n",
      "|MATRICULA;TIPO_ADMISSAO;TIPO_ENSINO_MEDIO;POLITICA_AFIRMATIVA_ALUNOS|\n",
      "+--------------------------------------------------------------------+\n",
      "|                                                102210009;vestibu...|\n",
      "|                                                104210033;vestibu...|\n",
      "|                                                106110136;vestibu...|\n",
      "|                                                106110805;vestibu...|\n",
      "|                                                108110007;vestibu...|\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 3761\n",
      "Erro ao salvar a tabela tabela_dados_ingresso: An error occurred while calling o361.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: tabela_dados_pessoais\n",
      "Verificando DataFrame: tabela_dados_pessoais\n",
      "root\n",
      " |-- MATRICULA;IDADE;GENERO;LOCAL_NASCIMENTO;ESTADO: string (nullable = true)\n",
      "\n",
      "+----------------------------------------------+\n",
      "|MATRICULA;IDADE;GENERO;LOCAL_NASCIMENTO;ESTADO|\n",
      "+----------------------------------------------+\n",
      "|                          104110028;39;masc...|\n",
      "|                          104210006;38;femi...|\n",
      "|                          105110106;36;masc...|\n",
      "|                          107210223;37;femi...|\n",
      "|                          108210203;35;masc...|\n",
      "+----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 3761\n",
      "Erro ao salvar a tabela tabela_dados_pessoais: An error occurred while calling o382.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: tabela_disciplinas\n",
      "Verificando DataFrame: tabela_disciplinas\n",
      "root\n",
      " |-- ANO;CODE;DISCIPLINA;ESTADO;CREDITO: string (nullable = true)\n",
      "\n",
      "+----------------------------------+\n",
      "|ANO;CODE;DISCIPLINA;ESTADO;CREDITO|\n",
      "+----------------------------------+\n",
      "|              1999;1411183;labo...|\n",
      "|              2023;1109049;?lge...|\n",
      "|              2023;1411332;m?to...|\n",
      "|              2023;1411337;prov...|\n",
      "|              1990;1411052;ling...|\n",
      "+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 690\n",
      "Erro ao salvar a tabela tabela_disciplinas: An error occurred while calling o403.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n",
      "Processando tabela: tabela_motivo_evasao\n",
      "Verificando DataFrame: tabela_motivo_evasao\n",
      "root\n",
      " |-- MATRICULA;RAZAO_DE_INATIVIDADE_DE_ALUNO: string (nullable = true)\n",
      "\n",
      "+---------------------------------------+\n",
      "|MATRICULA;RAZAO_DE_INATIVIDADE_DE_ALUNO|\n",
      "+---------------------------------------+\n",
      "|                   107210010;desconh...|\n",
      "|                     109210017;abandono|\n",
      "|                     112150428;expulsao|\n",
      "|                   112210115;desconh...|\n",
      "|                   112210439;desconh...|\n",
      "+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de linhas: 3761\n",
      "Erro ao salvar a tabela tabela_motivo_evasao: An error occurred while calling o424.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicando as transformações nas tabelas\n",
    "for table, df in dataframes.items():\n",
    "    print(f\"Processando tabela: {table}\")\n",
    "    df = handle_missing_values(df, strategy=\"mean\")  # Tratamento de valores ausentes\n",
    "    for col_name in df.columns:\n",
    "        if df.schema[col_name].dataType in ['int', 'double', 'float']:\n",
    "            df = remove_outliers(df, col_name)  # Remoção de outliers\n",
    "        else:\n",
    "            df = standardize_categories(df, col_name)  # Normalização de categorias\n",
    "    \n",
    "    df = df.dropDuplicates()  # Remoção de duplicatas\n",
    "\n",
    "    # Verificações antes de salvar\n",
    "    print(f\"Verificando DataFrame: {table}\")\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    print(f\"Número de linhas: {df.count()}\")\n",
    "\n",
    "    # Salvando o DataFrame\n",
    "    try:\n",
    "        df.write.mode(\"overwrite\").csv(f\"file:///E:/Mestrado UFCG/Semestre 2024.2/Dados/Tabelas_0/cleaned_{table}\", header=True)\n",
    "        print(f\"Tabela {table} salva com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar a tabela {table}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0242a5-b7d1-4bf5-9fb5-f8e352af210c",
   "metadata": {},
   "source": [
    "### Visualizar tabelas pós limpeza ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a63694-41c5-491d-a37c-4785c4c4fce0",
   "metadata": {},
   "source": [
    "### Visualização com PySpark ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8ef6ba-8562-4b43-bb8e-5542012a22dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|MATRICULA;RAZAO_DE_INATIVIDADE_DE_ALUNO|\n",
      "+---------------------------------------+\n",
      "|                   107210010;desconh...|\n",
      "|                     109210017;abandono|\n",
      "|                     112150428;expulsao|\n",
      "|                   112210115;desconh...|\n",
      "|                   112210439;desconh...|\n",
      "|                     113111428;abandono|\n",
      "|                     113111803;expulsao|\n",
      "|                   113210893;desconh...|\n",
      "|                   114110469;desconh...|\n",
      "|                   114111357;desconh...|\n",
      "+---------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- MATRICULA;RAZAO_DE_INATIVIDADE_DE_ALUNO: string (nullable = true)\n",
      "\n",
      "+-------+---------------------------------------+\n",
      "|summary|MATRICULA;RAZAO_DE_INATIVIDADE_DE_ALUNO|\n",
      "+-------+---------------------------------------+\n",
      "|  count|                                   3761|\n",
      "|   mean|                                   NULL|\n",
      "| stddev|                                   NULL|\n",
      "|    min|                   102210001;cancela...|\n",
      "|    max|                   123211071;desconh...|\n",
      "+-------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibir as primeiras 10 linhas de um DataFrame\n",
    "df.show(10)\n",
    "\n",
    "# Exibir o schema do DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Exibir estatísticas descritivas\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec911944-ac63-4709-9dec-d918639ad58c",
   "metadata": {},
   "source": [
    "### Converter DataFrame do PySpark para Pandas ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476084a4-5d2e-4232-a1cf-34009d47233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MATRICULA;RAZAO_DE_INATIVIDADE_DE_ALUNO'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(pandas_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e21a16c-6d6e-4bac-8731-40e4eaf6ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MATRICULA;RAZAO_DE_INATIVIDADE_DE_ALUNO\n",
      "0                  107210010;desconhecido\n",
      "1                      109210017;abandono\n",
      "2                      112150428;expulsao\n",
      "3                  112210115;desconhecido\n",
      "4                  112210439;desconhecido\n"
     ]
    }
   ],
   "source": [
    "# Converter DataFrame do PySpark para Pandas\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Exibir as primeiras linhas no Pandas\n",
    "print(pandas_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b836db-6964-4072-a805-585169a96c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\big data\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd47b0-3a7b-4e45-bd66-d47c0a6bff91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
